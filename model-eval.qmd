---
title: "Which AI model writes the best R code?"
format: html
execute: 
    echo: false
    warning: false
knitr: 
  opts_chunk:
    message: false
---

```{r}
library(tidyverse)
library(vitals)
library(gt)
```

```{r}
are_files <- list.files(here::here("results_rda"), full.names = TRUE)

for (file in are_files) {
  load(file)
}
```

```{r}
are_eval <- 
  vitals_bind(
    `Claude Sonnet 3.7\n(No Thinking)` = sonnet_3_7,
    `Claude Sonnet 3.7\n(Thinking)` = sonnet_3_7_thinking,
    `GPT-4.1` = gpt_4_1,
    `o1` = gpt_o1,
    `o3-mini` = gpt_o3_mini,
    `o3` = gpt_o3,
    `o4-mini` = gpt_o4_mini
  ) |> 
  rename(model = task) |> 
  mutate(
    model = fct_reorder(model, score, .fun = \(x) sum(x == "C", na.rm = TRUE)),
    score = fct_recode(
      score, 
      "Correct" = "C", "Partially Correct" = "P", "Incorrect" = "I"
  )
)
```


**LLMs can now help you write R code. There are many available models, so which one should you pick?**

We looked at a handful of models and evaluated how well they each generate R code. To do so, we used the [vitals package](https://vitals.tidyverse.org/), a framework for LLM evaluation. vitals contains functions for measuring the effectiveness of an LLM, as well as a dataset of challenging R coding problems and their solutions. We evaluated model performance on this set of coding problems. 

## Current recommendation: OpenAI o3, OpenAI o4-mini, or Claude Sonnet 3.7

```{r}
are_eval |> 
  ggplot(aes(y = model, fill = score)) +
  geom_bar(position = "fill") +
  scale_fill_manual(
    breaks = rev,
    values = 
      c(
        "Correct" = "#6caea7",
        "Partially Correct" = "#f6e8c3", 
        "Incorrect" = "#ef8a62"
      )
  ) +
  scale_x_continuous(labels = scales::percent) +
  labs(
    x = "Percent", 
    y = NULL,
    title = "Model performance on R code generation",
    fill = "Score",
    subtitle = 
      "OpenAI's newest reasoning models, o3 and o4-mini, are a substantial improvement on their\nprevious generation of models. Claude 3.7 Sonnet trails these new models slightly."
  ) +
  theme(
    plot.subtitle = element_text(face = "italic"),
    legend.position = "bottom"
  ) 
```

For R coding tasks, we recommend using OpenAI o3 or o4-mini or Anthropic's Claude Sonnet 3.7. Many R programmers seem to prefer Claude Sonnet and it remains a good solution for R code generation, even though o3 and o4-mini slightly beat it out in this evaluation. 

::: {.callout-note}
## Reasoning vs. non-reasoning models
_Thinking_ or _reasoning_ models are LLMs that attempt to solve tasks through structured, step-by-step processing rather than just pattern-matching.  

Most of the models we looked at here are reasoning models, or are capable of reasoning. The only models not designed for reasoning are GPT-4.1 and Claude Sonnet 3.7 with thinking disabled.
:::


## Key insights

* OpenAI's o3 and o4-mini and Anthropic's Claude Sonnet 3.7 are the current best performers on the set of R coding tasks. o3 and o4-mini, released in April 2025, are OpenAI's newest models.
* Claude Sonnet 3.7 performed similarly regardless of whether thinking was enabled. 
* o3 and o4-mini performed much better than the previous generation of reasoning models, o1 and o3-mini, which were released in December 2024 and January 2025, respectively.

## Pricing

LLM pricing is typically provided per million tokens. Note that in our analysis, o3 and o4-mini performed similarly for R code generation, but o3 is about ten times more expensive. OpenAI uses the "mini" suffix for models that are smaller, faster, and cheaper than the other models. 

```{r pricing}
model_order <-
  are_eval$model  |> 
  fct_collapse(
    "Claude 3.7 Sonnet" = 
      c("Claude Sonnet 3.7\n(No Thinking)", "Claude Sonnet 3.7\n(Thinking)")
  ) |> 
  levels()

tribble(
  ~Name, ~Input, ~Output,
  "Claude 3.7 Sonnet", "$3.00", "$15.00",
  "o3", "$10.00", "$40.00",
  "o4-mini", "$1.10", "$4.40",
  "GPT-4.1", "$2.00", "$8.00",
  "o1", "$15.00", "$60.00",
  "o3-mini", "$1.10", "$4.40"
) |>
  arrange(desc(match(Name, model_order))) |> 
  gt() |>
  cols_label(
    Name = "Model",
    Input = "Input",
    Output = "Output"
  ) |>
  tab_header(title = "Price per 1 million tokens")
```

```{r}
are_costs <-
  bind_rows(
    gpt_4_1$get_cost(),
    gpt_o1$get_cost(),
    gpt_o3_mini$get_cost(),
    gpt_o3$get_cost(),
    gpt_o4_mini$get_cost()
  ) |> 
  filter(provider != "Anthropic") |>  # remove solver 
  bind_rows(
    sonnet_3_7$get_cost(),
    sonnet_3_7_thinking$get_cost()
  ) |> 
  summarize(
    min_input = min(input),
    max_input = max(input),
    min_output = min(output),
    max_output = max(output),
    total_cost = sum(str_extract(price, "\\d+") |> as.numeric())
  ) |> 
  mutate(
    across(min_input:max_output, \(x) signif(x, 4) |> format(big.mark = ","))
  )
```


A **token** is the fundmental unit of data that an LLM can process. For text processing, you can think of a token as approximately a word, although it may be smaller. 

In our evaluation process, each model used between `r are_costs$min_input` and `r are_costs$max_input` input tokens and between `r are_costs$min_output` and `r are_costs$max_output` output tokens. The entire analysis cost around $`r are_costs$total_cost`.


## Methodology

* We used [ellmer](https://ellmer.tidyverse.org/) to create connections to the various models and [vitals](https://vitals.tidyverse.org/) to evaluate model performance on R code generation tasks.
* We tested each model on a shared benchmark: the `are` dataset ("**A**n **R** **E**val"). `are` contains a collection of difficult R coding problems and a column, `target`, with information about the target solution.  
* Using vitals, we had each model solve each problem in `are`. Then, we scored their solutions using a scoring model (Claude Sonnet 3.7). Each solution received either an Incorrect, Partially Correct, or Correct score. 

You can see all the code used to evaluate the models [here](https://github.com/skaltman/model-eval-r/blob/99b405a40b6b5f12ed03eba6d1f6d3d20620cd84/eval.R). If you'd like to see a more in-depth analysis, check out Simon Couch's series of blog posts, which this post is based on, including [Evaluating o3 and o4-mini on R coding performance](https://www.simonpcouch.com/blog/2025-04-18-o3-o4-mini/). 
